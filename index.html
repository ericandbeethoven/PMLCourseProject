<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Course Project Writeup by ericandbeethoven</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Course Project Writeup</h1>
      <h2 class="project-tagline">Coursera Practical Machine Learning</h2>
    </section>

    <section class="main-content">
      <h3>
<a id="the-course-project-problem" class="anchor" href="#the-course-project-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Course Project Problem.</h3>

<p>The goal of your project is to predict the manner in which six healthy human subjects wearing accelerometers
perform the unilateral dumbbell biceps curl weight lifting exercise. </p>

<p>The participants were instructed to perform the exercise in one of five different fashions:</p>

<ul>
<li>exactly according to the specification (Class A), </li>
<li>throwing the elbows to the front (Class B), </li>
<li>lifting the dumbbell only halfway (Class C), </li>
<li>lowering the dumbbell only halfway (Class D) </li>
<li>and throwing the hips to the front (Class E).</li>
</ul>

<p>This dependent variable is the "classe" variable in the training set. </p>

<p>The project deliverables are this report describing how the model was built, how cross validation was used, what the expected out of sample error is, why the choices for all the above were made and a final prediction on the 20 test cases. </p>

<h3>
<a id="the-data" class="anchor" href="#the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Data</h3>

<p>The data was sourced from <a href="http://groupware.les.inf.puc-rio.br/har">this site</a>.</p>

<pre><code>train = read.csv("pml-training.csv", header=T, stringsAsFactors=F, na.strings = c("NA", "#DIV/0!"))
test = read.csv("pml-testing.csv",header=T, stringsAsFactors=F, na.strings = c("NA", "#DIV/0!"))
</code></pre>

<h3>
<a id="data-cleansing-and-feature-variable-selection" class="anchor" href="#data-cleansing-and-feature-variable-selection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Cleansing and Feature Variable Selection</h3>

<p>The first step was to understand the data properties and eliminate quirks or unnecessary variables that would have no explanatory value.</p>

<p>The following variables were eliminated:</p>

<ul>
<li>Statistical and summary measures were derived from other variables and therefore contained no additional explanatory value.</li>
<li>Raw and cvtd timestamps contained no additional explanatory value since other variables measured acceleration and thus contained time element.</li>
<li>new_window, num_window and other row indexes have no explanatory value as new_window has zero variability once the statistical and summary measures were eliminated and row indexes are merely index counter identification values.</li>
</ul>

<p>There were no missing or NA values after the above variables were eliminated so no additional data cleansing was required.</p>

<p>The dependent variable 'classe' and independent variable 'user_name' were both converted to factor variables for ease of use with the caret library train function.</p>

<pre><code>all$classe = as.factor(all$classe)
all$user_name = as.factor(all$user_name)
</code></pre>

<p>The resulting 'train' data frame was 19622 obs of 55 variables. </p>

<h3>
<a id="feature-selection" class="anchor" href="#feature-selection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Selection</h3>

<p>While it is best to err on over creation of features but first consider removing zero covariates.</p>

<pre><code>nsv = nearZeroVar(train.training[,4:54], saveMetrics = TRUE)
</code></pre>

<p>There were no near zero covariates identified for removal.</p>

<p>A Variable Importance exploratory analysis was run to gauge variable importance using a Random Forest model over a 20% split of the training data frame. The below plots illustrate the variable importance.</p>

<pre><code>inVarImp = createDataPartition(y = train.training$classe, p = 0.2, list = F)
varImpSub = train.training[inVarImp, ]
varImpSub = subset(varImpSub[3:55])
varImpRF = train(classe ~ ., data = varImpSub, method = "rf")

varImpObj &lt;- varImp(varImpRF)
plot(varImpObj, main = "Variable Importance of All Vars")
plot(varImpObj, main = "Variable Importance of Top 25 Vars", top = 25)
</code></pre>

<p><a href="http://rpubs.com/ebrucecfa/82744">Variable Importance of All Vars</a></p>

<p><a href="http://rpubs.com/ebrucecfa/82755">Variable Importance of Top 25 Vars</a></p>

<p>The variable importance exploratory analysis showed that 20-25 variables had the dominant explanatory value.</p>

<p>A final exploratory analysis to gauge likely best features based on high variability and select best pre-processing method was performed.</p>

<pre><code>train.training.mean = sapply(train.training[,4:54],mean,na.rm=1)
train.training.min = sapply(train.training[,4:54],min,na.rm=1)
train.training.max = sapply(train.training[,4:54],max,na.rm=1)
train.training.sd = sapply(train.training[,4:54],sd,na.rm=1)
train.training.summarystats = data.frame(train.training.min, 
                                     train.training.max, 
                                     train.training.mean,
                                     train.training.sd)
# sort by sd
train.training.summarystats = train.training.summarystats[order(-train.training.sd, train.training.mean ) , ]
head(train.training.summarystats)
</code></pre>

<p>magnet_forearm_y SD = 507.9857</p>

<p>magnet_arm_x SD =  444.7440</p>

<p>magnet_forearm_z SD = 371.8815</p>

<p>magnet_forearm_x SD = 347.5065</p>

<p>magnet_dumbbell_x SD = 338.4216</p>

<p>magnet_arm_z SD = 326.0821</p>

<p>As shown above, the final exploratory analysis showed a significant amount of variables exhibited exceptionally high variability.</p>

<p>It was thus decided to use all remaining 54 variables.</p>

<h3>
<a id="cross-validation" class="anchor" href="#cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cross Validation</h3>

<p>The data was split into Training and Probe Testing to get an idea of out of sample error rate. I decided to use training (60%), probe test (20%) &amp; validation (20%) from data set from train. This was recommended in Week 1 Prediction study design Slide 7/8.</p>

<pre><code>set.seed(123)
inTrain = createDataPartition(y = train$classe, p = 0.6, list = FALSE)
# 60% to training
train.training = train[inTrain,]
train.test = train[-inTrain,]

# 20% to test and 20% to validation fromt train.test
set.seed(123)
inTest = createDataPartition(y = train.test$classe, p = 0.5, list = FALSE)
train.testing = train.test[inTest,]
train.validate = train.test[-inTest,]
</code></pre>

<p>It was decided to perform repeated 10-fold cross validation with 10 repetitions. This would result in less bias, but more variance in prediction. A Center and Scale preprocessing was chosen given the exceptionally high variability exhibited in the chosen features.</p>

<h3>
<a id="algorithms" class="anchor" href="#algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithms</h3>

<p>Four Machine Learning Models were chosen.</p>

<ul>
<li>Random Forest Model (rf)</li>
<li>Generalized Boosted Regression Model (gbm)</li>
<li>Learning Vector Quantization Model (lvq)</li>
<li>Support Vector Machine Model (svm)</li>
</ul>

<p>The caret package train function was used to preprocess and perform the repeated cross validation.</p>

<pre><code>pargs = classe ~ .
preProc = c("center","scale")
ctrl = trainControl(method="repeatedcv",          # use repeated 10 fold cross validation
                 repeats=10,                          # do 10 repetitions of 10-fold cv
                 number=10)

# BOOSTED TREE GBM MODEL                                        
set.seed(123)
registerDoParallel(4)                               # Registrer a parallel backend for train
getDoParWorkers()
system.time(gbm.tune1010 &lt;- train(pargs,
                          data = train.training,
                          method = "gbm",
                          trControl = ctrl,
                          verbose=T)
)

#  Learning Vector Quantization MODEL
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
lvq.tune1010 &lt;- train(pargs,
               data = train.training,
               method = "lvq",
               preProc = preProc,
               trControl=ctrl)          # same as for gbm above
)   

#  Support Vector Machine SVM MODEL
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
svm.tune1010 &lt;- train(pargs,
                    data = train.training,
                    method = "svmRadial",
                    preProc = preProc,
                    trControl=ctrl)          # same as for gbm above
)   

# RANDOM FOREST MODEL
set.seed(123)
registerDoParallel(4,cores=4)
getDoParWorkers()
system.time(
rf.tune1010 &lt;- train(pargs,
               data = train.training,
               method = "rf",
               nodesize = 5,    
               ntree=500,
               preProc = preProc,
               trControl=ctrl)          # same as for gbm above
 )  
</code></pre>

<p>The performance metric for the comparison is Model Accuracy. From examining the boxplots of the sampling distributions for the three best models it is apparent that, in this case, the rf and gbm have the advantage.</p>

<pre><code>rValues1010 &lt;- resamples(list(gbm=gbm.tune1010, rf=rf.tune1010,  svm=svm.tune1010))
rValues1010$values

# BOXPLOTS COMPARING RESULTS
bwplot(rValues1010,metric="Accuracy")       # boxplot
</code></pre>

<p><a href="http://rpubs.com/ebrucecfa/83177">Machine Learning Model Comparison Boxplot</a></p>

<p>The lvq Optimal Model Accuracy = 0.6714843 with SD =  0.02271624 in contrast. It was eliminated from further consideration.</p>

<p>The predictions were then made against the train.training data frame using rf and gbm.</p>

<pre><code># Predict
gbm.tune.pred = predict(gbm.tune1010, newdata=train.testing)
rf.tune.pred = predict(rf.tune1010, newdata=train.testing, type="raw")
</code></pre>

<h3>
<a id="expected-out-of-sample-error" class="anchor" href="#expected-out-of-sample-error" aria-hidden="true"><span class="octicon octicon-link"></span></a>Expected Out of Sample Error</h3>

<p>The train.validate data frame was used to calculate the expected out of sample error.</p>

<pre><code># rf
rf.tune.pred.oos = predict(rf.tune1010, newdata=train.validate, type="raw")
missClass = function(values, rf.tune.pred.oos) {
  sum(rf.tune.pred.oos != values)/length(values)
}  
rf.oos.error = missClass(train.validate$classe, rf.tune.pred.oos)
# gbm
gbm.tune.pred.oos = predict(gbm.tune1010, newdata=train.validate, type="raw")
missClass = function(values, gbm.tune.pred.oos) {
sum(gbm.tune.pred.oos != values)/length(values)
}  
gbm.oos.error = missClass(train.validate$classe, gbm.tune.pred.oos)
</code></pre>

<p>Based on the misclassification rate on the validation subset, an unbiased estimate of the rf out of sample error = 1.0706% and the gbm out of sample error = 4.4099%</p>

      <footer class="site-footer">

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>

