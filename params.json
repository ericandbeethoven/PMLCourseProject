{"name":"Ericandbeethoven.GitHub.io","tagline":"Course Project for Coursera Practical Machine Learning","body":"### The Course Project Problem.\r\nThe goal of your project is to predict the manner in which six healthy human subjects wearing accelerometers\r\nperform the unilateral dumbbell biceps curl weight lifting exercise. \r\n\r\nThe participants were instructed to perform the exercise in one of five different fashions:\r\n* exactly according to the specification (Class A), \r\n* throwing the elbows to the front (Class B), \r\n* lifting the dumbbell only halfway (Class C), \r\n* lowering the dumbbell only halfway (Class D) \r\n* and throwing the hips to the front (Class E).\r\n\r\nThis dependent variable is the \"classe\" variable in the training set. \r\n\r\nThe project deliverables are this report describing how the model was built, how cross validation was used, what the expected out of sample error is, and why the choices for all the above were made. \r\n\r\n### The Data\r\nThe data was sourced from [this site](http://groupware.les.inf.puc-rio.br/har).\r\n\r\nThe below R commands and parameters were used.\r\n\r\n    train = read.csv(\"pml-training.csv\", header=T, stringsAsFactors=F, na.strings = c(\"NA\", \"#DIV/0!\"))\r\n    test = read.csv(\"pml-testing.csv\",header=T, stringsAsFactors=F, na.strings = c(\"NA\", \"#DIV/0!\"))\r\n\r\n### Data Cleansing and Feature Variable Selection\r\nThe first step was to understand the data properties and eliminate quirks or unnecessary variables that would have no explanatory value.\r\n\r\nThe following variables were eliminated:\r\n* Statistical and summary measures were derived from other variables and therefore contained no additional explanatory value.\r\n* Raw and cvtd timestamps contained no additional explanatory value since other variables measured acceleration and thus contained time element.\r\n* new_window & num_window have no explanatory value as new_window has zero variability once the statistical and summary measures were eliminated and num_window is merely an index counter identification value.\r\n\r\nThere were no missing or NA values after the above variables were eliminated so no additional data cleansing was required.\r\n\r\nThe dependent variable 'classe' and independent variable 'user_name' were both converted to factor variables for ease of use with the caret library train function.\r\n\r\nThe resulting 'train' data frame was 19622 obs of 55 variables. \r\n\r\n### Cross Validation\r\nThe data was split into Training and Probe Testing to get an idea of out of sample error rate. I decided to use training (60%), probe test (20%) & validation (20%) from data set from train. This was recommended in Week 1 Prediction study design Slide 7/8.\r\n\r\n    set.seed(123)\r\n    inTrain = createDataPartition(y = train$classe, p = 0.6, list = FALSE)\r\n    # 60% to training\r\n    train.training = train[inTrain,]\r\n    train.test = train[-inTrain,]\r\n\r\n    # 20% to test and 20% to validation fromt train.test\r\n    set.seed(123)\r\n    inTest = createDataPartition(y = train.test$classe, p = 0.5, list = FALSE)\r\n    train.testing = train.test[inTest,]\r\n    train.validate = train.test[-inTest,]\r\n\r\n## Feature Selection\r\nWhile it is best to err on over creation of features but first consider removing zero covariates.\r\n\r\n    nsv = nearZeroVar(train.training[,4:54], saveMetrics = TRUE)\r\n\r\nThere were no near zero covariates identified for removal.\r\n\r\nAn exploratory analysis was run to gauge variable importance using a Random Forest model over a 20% split of the training data frame. The below plots illustrate the variable importance.\r\n\r\n![Variable Importance of All Vars](http://rpubs.com/ebrucecfa/82744)\r\n![Variable Importance of Top 25 Vars](http://rpubs.com/ebrucecfa/82755)\r\n\r\nThe variable importance exploratory analysis showed that 20-25 variables had the dominant explanatory value.\r\n\r\nA final exploratory analysis to gauge likely best features and select best pre-processing method was performed.\r\n \r\n                  train.training.min train.training.max train.training.mean train.training.sd\r\nmagnet_forearm_y                -896               1480            385.3510          507.9857\r\nmagnet_arm_x                    -580                780            190.8281          444.7440\r\nmagnet_forearm_z                -973               1090            392.8455          371.8815\r\nmagnet_forearm_x               -1280                663           -312.5898          347.5065\r\nmagnet_dumbbell_x               -639                584           -330.0082          338.4216\r\nmagnet_arm_z                    -595                694            306.6091          326.0821\r\n\r\nAs shown above, the final exploratory analysis showed a significant amount of variables exhibited exceptionally high variability.\r\n\r\nIt was thus decided to use all remaining 54 variables and perform repeated 10-fold cross validation with 10 repetitions. This would result in less bias, but more variance in prediction. A Center and Scale preprocessing was chosen given the exceptionally high variability.\r\n\r\n### Algorithms\r\nFour Machine Learning Models were chosen.\r\n* Random Forest Model (rf)\r\n* Generalized Boosted Regression Model (gbm)\r\n* Learning Vector Quantization Model (lvq)\r\n* Support Vector Machine Model (svm)\r\n\r\nThe performance metric for the comparison is Model Accuracy. From examining the boxplots of the sampling distributions for the three best models it is apparent that, in this case, the rf and gbm have the advantage.\r\n\r\n[Machine Learning Model Comparison](http://rpubs.com/ebrucecfa/82645)\r\n\r\nThe lvq Optimal Model Accuracy = 0.7386467 with SD =  0.02004137 in contrast. It was eliminated from further consideration.\r\n\r\n### Expected Out of Sample Error ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}