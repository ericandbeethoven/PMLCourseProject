{"name":"Course Project Writeup","tagline":"Coursera Practical Machine Learning","body":"### The Course Project Problem.\r\nThe goal of your project is to predict the manner in which six healthy human subjects wearing accelerometers\r\nperform the unilateral dumbbell biceps curl weight lifting exercise. \r\n\r\nThe participants were instructed to perform the exercise in one of five different fashions:\r\n* exactly according to the specification (Class A), \r\n* throwing the elbows to the front (Class B), \r\n* lifting the dumbbell only halfway (Class C), \r\n* lowering the dumbbell only halfway (Class D) \r\n* and throwing the hips to the front (Class E).\r\n\r\nThis dependent variable is the \"classe\" variable in the training set. \r\n\r\nThe project deliverables are this report describing how the model was built, how cross validation was used, what the expected out of sample error is, why the choices for all the above were made and a final prediction on the 20 test cases. \r\n\r\n### The Data\r\nThe data was sourced from [this site](http://groupware.les.inf.puc-rio.br/har).\r\n\r\n    train = read.csv(\"pml-training.csv\", header=T, stringsAsFactors=F, na.strings = c(\"NA\", \"#DIV/0!\"))\r\n    test = read.csv(\"pml-testing.csv\",header=T, stringsAsFactors=F, na.strings = c(\"NA\", \"#DIV/0!\"))\r\n\r\n### Data Cleansing and Feature Variable Selection\r\nThe first step was to understand the data properties and eliminate quirks or unnecessary variables that would have no explanatory value.\r\n\r\nThe following variables were eliminated:\r\n* Statistical and summary measures were derived from other variables and therefore contained no additional explanatory value.\r\n* Raw and cvtd timestamps contained no additional explanatory value since other variables measured acceleration and thus contained time element.\r\n* new_window, num_window and other row indexes have no explanatory value as new_window has zero variability once the statistical and summary measures were eliminated and row indexes are merely index counter identification values.\r\n\r\nThere were no missing or NA values after the above variables were eliminated so no additional data cleansing was required.\r\n\r\nThe dependent variable 'classe' and independent variable 'user_name' were both converted to factor variables for ease of use with the caret library train function. Random Forest method requires the dependent variable be stored as a factor variable when training a model for classification.\r\n\r\n    all$classe = as.factor(all$classe)\r\n    all$user_name = as.factor(all$user_name)\r\n\r\nThe resulting 'train' data frame was 19622 obs of 55 variables. \r\n\r\n### Feature Selection\r\nWhile it is best to err on over creation of features, the first step was to consider removing zero covariates.\r\n\r\n    nsv = nearZeroVar(train.training[,4:54], saveMetrics = TRUE)\r\n\r\nThere were no near zero covariates identified for removal.\r\n\r\nA Variable Importance exploratory analysis was run to gauge variable importance using a Random Forest model over a 20% split of the training data frame.\r\n\r\n    inVarImp = createDataPartition(y = train.training$classe, p = 0.2, list = F)\r\n    varImpSub = train.training[inVarImp, ]\r\n    varImpSub = subset(varImpSub[3:55])\r\n    varImpRF = train(classe ~ ., data = varImpSub, method = \"rf\")\r\n\r\n    varImpObj <- varImp(varImpRF)\r\n    plot(varImpObj, main = \"Variable Importance of All Vars\")\r\n    plot(varImpObj, main = \"Variable Importance of Top 25 Vars\", top = 25)\r\n\r\nThe below plots illustrate the variable importance.\r\n\r\n[Variable Importance of All Vars](http://rpubs.com/ebrucecfa/82744)\r\n\r\n[Variable Importance of Top 25 Vars](http://rpubs.com/ebrucecfa/82755)\r\n\r\nThe variable importance exploratory analysis showed that 20-25 variables had the dominant explanatory value.\r\n\r\nA final exploratory analysis to gauge likely best features based on high variability and select best pre-processing method was performed.\r\n\r\n    train.training.mean = sapply(train.training[,4:54],mean,na.rm=1)\r\n    train.training.min = sapply(train.training[,4:54],min,na.rm=1)\r\n    train.training.max = sapply(train.training[,4:54],max,na.rm=1)\r\n    train.training.sd = sapply(train.training[,4:54],sd,na.rm=1)\r\n    train.training.summarystats = data.frame(train.training.min, \r\n                                         train.training.max, \r\n                                         train.training.mean,\r\n                                         train.training.sd)\r\n    # sort by sd\r\n    train.training.summarystats = train.training.summarystats[order(-train.training.sd, train.training.mean ) , ]\r\n    head(train.training.summarystats)\r\n\r\nmagnet_forearm_y SD = 507.9857\r\n\r\nmagnet_arm_x SD =  444.7440\r\n\r\nmagnet_forearm_z SD = 371.8815\r\n\r\nmagnet_forearm_x SD = 347.5065\r\n\r\nmagnet_dumbbell_x SD = 338.4216\r\n\r\nmagnet_arm_z SD = 326.0821\r\n\r\nAs illustrated above, the final exploratory analysis showed a significant amount of variables exhibited exceptionally high variability.\r\n\r\nAfter all the feature engineering and analysis, it was decided to use all remaining 54 variables.\r\n\r\n### Cross Validation\r\nThe data was split into Training and Probe Testing to get an idea of out of sample error rate. I decided to use training (60%), probe test (20%) & validation (20%) from data set from train. This was recommended in Week 1 Prediction study design Slide 7/8.\r\n\r\n    set.seed(123)\r\n    inTrain = createDataPartition(y = train$classe, p = 0.6, list = FALSE)\r\n    # 60% to training\r\n    train.training = train[inTrain,]\r\n    train.test = train[-inTrain,]\r\n\r\n    # 20% to test and 20% to validation fromt train.test\r\n    set.seed(123)\r\n    inTest = createDataPartition(y = train.test$classe, p = 0.5, list = FALSE)\r\n    train.testing = train.test[inTest,]\r\n    train.validate = train.test[-inTest,]\r\n\r\nIt was decided to perform repeated 10-fold cross validation with 10 repetitions. This would result in less bias, but more variance in prediction. A Center and Scale preprocessing was chosen given the exceptionally high variability exhibited in the chosen features.\r\n\r\n### Algorithms\r\nFour Machine Learning Models were chosen.\r\n* Random Forest Model (rf)\r\n* Generalized Boosted Regression Model (gbm)\r\n* Learning Vector Quantization Model (lvq)\r\n* Support Vector Machine Model (svm)\r\n\r\nThe caret package train function was used to preprocess and perform the repeated cross validation.\r\n\r\n    pargs = classe ~ .\r\n    preProc = c(\"center\",\"scale\")\r\n    ctrl = trainControl(method=\"repeatedcv\",          # use repeated 10 fold cross validation\r\n                     repeats=10,                          # do 10 repetitions of 10-fold cv\r\n                     number=10)\r\n\r\n    # BOOSTED TREE GBM MODEL\t\t\t\t\t\t\t\t\t\t\r\n    set.seed(123)\r\n    registerDoParallel(4)\t\t                        # Registrer a parallel backend for train\r\n    getDoParWorkers()\r\n    system.time(gbm.tune1010 <- train(pargs,\r\n                              data = train.training,\r\n                              method = \"gbm\",\r\n                              trControl = ctrl,\r\n                              verbose=T)\r\n    )\r\n\r\n    #  Learning Vector Quantization MODEL\r\n    set.seed(123)\r\n    registerDoParallel(4,cores=4)\r\n    getDoParWorkers()\r\n    system.time(\r\n    lvq.tune1010 <- train(pargs,\r\n                   data = train.training,\r\n                   method = \"lvq\",\r\n                   preProc = preProc,\r\n                   trControl=ctrl)\t        # same as for gbm above\r\n    )\t\r\n\r\n    #  Support Vector Machine SVM MODEL\r\n    set.seed(123)\r\n    registerDoParallel(4,cores=4)\r\n    getDoParWorkers()\r\n    system.time(\r\n    svm.tune1010 <- train(pargs,\r\n                        data = train.training,\r\n                        method = \"svmRadial\",\r\n                        preProc = preProc,\r\n                        trControl=ctrl)          # same as for gbm above\r\n    )\t\r\n\r\n    # RANDOM FOREST MODEL\r\n    set.seed(123)\r\n    registerDoParallel(4,cores=4)\r\n    getDoParWorkers()\r\n    system.time(\r\n    rf.tune1010 <- train(pargs,\r\n                   data = train.training,\r\n                   method = \"rf\",\r\n                   nodesize = 5,  \t\r\n                   ntree=500,\r\n                   preProc = preProc,\r\n                   trControl=ctrl)\t        # same as for gbm above\r\n     )\t\r\n\r\nThe performance metric for the comparison is Model Accuracy. From examining the boxplots of the sampling distributions for the three best models it is apparent that, in this case, the rf and gbm have the advantage.\r\n\r\n    rValues1010 <- resamples(list(gbm=gbm.tune1010, rf=rf.tune1010,  svm=svm.tune1010))\r\n    rValues1010$values\r\n\r\n    # BOXPLOTS COMPARING RESULTS\r\n    bwplot(rValues1010,metric=\"Accuracy\")\t\t# boxplot\r\n\r\n[Machine Learning Model Comparison Boxplot](http://rpubs.com/ebrucecfa/83177)\r\n\r\nThe lvq Optimal Model Accuracy = 0.6714843 with SD =  0.02271624 in contrast. It was eliminated from further consideration.\r\n\r\nThe predictions were then made against the train.training data frame using rf and gbm.\r\n\r\n    # Predict\r\n    gbm.tune.pred = predict(gbm.tune1010, newdata=train.testing)\r\n    rf.tune.pred = predict(rf.tune1010, newdata=train.testing, type=\"raw\")\r\n\r\n### Expected Out of Sample Error \r\nThe train.validate data frame was used to calculate the expected out of sample error.\r\n\r\n    # rf\r\n    rf.tune.pred.oos = predict(rf.tune1010, newdata=train.validate, type=\"raw\")\r\n    missClass = function(values, rf.tune.pred.oos) {\r\n      sum(rf.tune.pred.oos != values)/length(values)\r\n    }  \r\n    rf.oos.error = missClass(train.validate$classe, rf.tune.pred.oos)\r\n    # gbm\r\n    gbm.tune.pred.oos = predict(gbm.tune1010, newdata=train.validate, type=\"raw\")\r\n    missClass = function(values, gbm.tune.pred.oos) {\r\n    sum(gbm.tune.pred.oos != values)/length(values)\r\n    }  \r\n    gbm.oos.error = missClass(train.validate$classe, gbm.tune.pred.oos)\r\n\r\nBased on the misclassification rate on the validation subset, an unbiased estimate of the rf out of sample error = 1.0706% and the gbm out of sample error = 4.4099%\r\n\r\nThe rf model was used to predict on the test set since it exhibited both the highest expected Accuracy and lowest expected Out of Sample Error.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}